"""
AI/ML Engine for Trading Analytics
Advanced machine learning models for price prediction, sentiment analysis, and risk assessment
"""

import asyncio
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple, Union
from dataclasses import dataclass
import logging
import pickle
import json
from pathlib import Path

# ML Libraries
import sklearn
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import xgboost as xgb
import numpy as np

# Deep Learning (Optional - requires tensorflow)
try:
    import tensorflow as tf
    from tensorflow.keras.models import Sequential, Model
    from tensorflow.keras.layers import LSTM, Dense, Dropout, Attention, Input
    from tensorflow.keras.optimizers import Adam

    TENSORFLOW_AVAILABLE = True
except ImportError:
    TENSORFLOW_AVAILABLE = False
    print("TensorFlow not available. Deep learning models will be disabled.")

# Sentiment Analysis
try:
    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
    import nltk

    SENTIMENT_AVAILABLE = True
except ImportError:
    SENTIMENT_AVAILABLE = False
    print("Sentiment analysis libraries not available.")

from src.utils.logger import get_logger
from src.utils.database import DatabaseManager
from src.analytics.influxdb_sync import influx_sync, MarketData


@dataclass
class MLPrediction:
    """ML prediction result."""

    symbol: str
    model_name: str
    prediction_type: str  # price, volatility, trend, sentiment
    predicted_value: float
    confidence: float
    timestamp: datetime
    features_used: List[str]
    model_accuracy: float


@dataclass
class TradingSignal:
    """Trading signal generated by ML models."""

    symbol: str
    signal_type: str  # buy, sell, hold
    strength: float  # 0-1
    confidence: float  # 0-1
    expected_return: float
    risk_score: float
    timestamp: datetime
    model_ensemble: List[str]


class AIMLEngine:
    """Advanced AI/ML engine for trading analytics."""

    def __init__(self, db_manager: DatabaseManager):
        self.db_manager = db_manager
        self.logger = get_logger(__name__)

        # Model storage
        self.models = {}
        self.scalers = {}
        self.model_performance = {}

        # Feature engineering
        self.feature_columns = []
        self.feature_importance = {}

        # Sentiment analyzer
        self.sentiment_analyzer = (
            SentimentIntensityAnalyzer() if SENTIMENT_AVAILABLE else None
        )

        # Model configurations
        self.model_configs = {
            "random_forest": {"n_estimators": 100, "max_depth": 10, "random_state": 42},
            "xgboost": {
                "n_estimators": 100,
                "max_depth": 6,
                "learning_rate": 0.1,
                "random_state": 42,
            },
            "lstm": {
                "sequence_length": 60,
                "lstm_units": 50,
                "dropout": 0.2,
                "epochs": 50,
                "batch_size": 32,
            },
            "transformer": {
                "sequence_length": 60,
                "d_model": 64,
                "num_heads": 8,
                "epochs": 30,
                "batch_size": 32,
            },
        }

        self.model_directory = Path("models/ai_ml")
        self.model_directory.mkdir(parents=True, exist_ok=True)

    # =====================================================
    # FEATURE ENGINEERING
    # =====================================================

    async def engineer_features(
        self, symbol: str, lookback_days: int = 252
    ) -> pd.DataFrame:
        """Engineer comprehensive features for ML models."""
        try:
            # Get base market data
            end_time = datetime.now()
            start_time = end_time - timedelta(days=lookback_days)

            # Fetch data from different sources
            stock_data = await self.db_manager.get_stock_data(
                symbol, start_time, end_time
            )

            if not stock_data:
                return pd.DataFrame()

            df = pd.DataFrame(stock_data)
            df["timestamp"] = pd.to_datetime(df["timestamp"])
            df.set_index("timestamp", inplace=True)
            df.sort_index(inplace=True)

            # Technical indicators
            df = self._add_technical_features(df)

            # Market microstructure features
            df = self._add_microstructure_features(df)

            # Time-based features
            df = self._add_temporal_features(df)

            # Volatility features
            df = self._add_volatility_features(df)

            # Market regime features
            df = await self._add_market_regime_features(df, symbol)

            # Sentiment features (if available)
            if SENTIMENT_AVAILABLE:
                df = await self._add_sentiment_features(df, symbol)

            # Cross-asset features
            df = await self._add_cross_asset_features(df, symbol)

            # Clean and prepare features
            df = self._clean_features(df)

            self.feature_columns = [
                col
                for col in df.columns
                if col not in ["close", "open", "high", "low", "volume"]
            ]

            return df

        except Exception as e:
            self.logger.error(f"Feature engineering failed for {symbol}: {e}")
            return pd.DataFrame()

    def _add_technical_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Add technical indicator features."""
        # Moving averages
        for period in [5, 10, 20, 50, 100, 200]:
            df[f"sma_{period}"] = df["close"].rolling(window=period).mean()
            df[f"ema_{period}"] = df["close"].ewm(span=period).mean()
            df[f"price_to_sma_{period}"] = df["close"] / df[f"sma_{period}"]

        # RSI
        delta = df["close"].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
        rs = gain / loss
        df["rsi_14"] = 100 - (100 / (1 + rs))

        # MACD
        ema_12 = df["close"].ewm(span=12).mean()
        ema_26 = df["close"].ewm(span=26).mean()
        df["macd"] = ema_12 - ema_26
        df["macd_signal"] = df["macd"].ewm(span=9).mean()
        df["macd_histogram"] = df["macd"] - df["macd_signal"]

        # Bollinger Bands
        df["bb_middle"] = df["close"].rolling(window=20).mean()
        bb_std = df["close"].rolling(window=20).std()
        df["bb_upper"] = df["bb_middle"] + (bb_std * 2)
        df["bb_lower"] = df["bb_middle"] - (bb_std * 2)
        df["bb_position"] = (df["close"] - df["bb_lower"]) / (
            df["bb_upper"] - df["bb_lower"]
        )

        # ATR
        high_low = df["high"] - df["low"]
        high_close = np.abs(df["high"] - df["close"].shift())
        low_close = np.abs(df["low"] - df["close"].shift())
        true_range = np.maximum(high_low, np.maximum(high_close, low_close))
        df["atr_14"] = true_range.rolling(window=14).mean()

        # Momentum indicators
        df["momentum_10"] = df["close"] / df["close"].shift(10) - 1
        df["rate_of_change_10"] = (
            (df["close"] - df["close"].shift(10)) / df["close"].shift(10) * 100
        )

        return df

    def _add_microstructure_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Add market microstructure features."""
        # Volume features
        df["volume_sma_20"] = df["volume"].rolling(window=20).mean()
        df["volume_ratio"] = df["volume"] / df["volume_sma_20"]
        df["volume_price_trend"] = (df["close"] - df["close"].shift(1)) * df["volume"]

        # Price action features
        df["body_size"] = np.abs(df["close"] - df["open"])
        df["upper_shadow"] = df["high"] - np.maximum(df["open"], df["close"])
        df["lower_shadow"] = np.minimum(df["open"], df["close"]) - df["low"]
        df["total_range"] = df["high"] - df["low"]

        # Gaps
        df["gap"] = df["open"] - df["close"].shift(1)
        df["gap_percentage"] = df["gap"] / df["close"].shift(1) * 100

        return df

    def _add_temporal_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Add time-based features."""
        df["hour"] = df.index.hour
        df["day_of_week"] = df.index.dayofweek
        df["month"] = df.index.month
        df["quarter"] = df.index.quarter

        # Market session features
        df["is_market_open"] = ((df["hour"] >= 9) & (df["hour"] < 16)).astype(int)
        df["is_pre_market"] = ((df["hour"] >= 4) & (df["hour"] < 9)).astype(int)
        df["is_after_hours"] = ((df["hour"] >= 16) & (df["hour"] < 20)).astype(int)

        return df

    def _add_volatility_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Add volatility-based features."""
        # Historical volatility
        returns = df["close"].pct_change()
        for period in [5, 10, 20, 60]:
            df[f"volatility_{period}"] = returns.rolling(window=period).std() * np.sqrt(
                252
            )

        # Parkinson volatility (uses high-low)
        df["parkinson_vol"] = np.sqrt(
            (1 / (4 * np.log(2))) * (np.log(df["high"] / df["low"])) ** 2
        )

        # Volatility rank
        df["vol_rank_60"] = (
            returns.rolling(window=60).std().rolling(window=252).rank(pct=True)
        )

        return df

    async def _add_market_regime_features(
        self, df: pd.DataFrame, symbol: str
    ) -> pd.DataFrame:
        """Add market regime and macro features."""
        try:
            # VIX data (if available)
            vix_data = await self.db_manager.get_market_indicator_data(
                "VIX", df.index[0], df.index[-1]
            )
            if vix_data:
                vix_df = pd.DataFrame(vix_data)
                vix_df["timestamp"] = pd.to_datetime(vix_df["timestamp"])
                vix_df.set_index("timestamp", inplace=True)
                df = df.join(
                    vix_df[["value"]].rename(columns={"value": "vix"}), how="left"
                )
                df["vix"].fillna(method="ffill", inplace=True)
                df["vix_rank"] = df["vix"].rolling(window=252).rank(pct=True)

            # Market breadth (using SPY as proxy)
            if symbol != "SPY":
                spy_data = await self.db_manager.get_stock_data(
                    "SPY", df.index[0], df.index[-1]
                )
                if spy_data:
                    spy_df = pd.DataFrame(spy_data)
                    spy_df["timestamp"] = pd.to_datetime(spy_df["timestamp"])
                    spy_df.set_index("timestamp", inplace=True)
                    spy_returns = spy_df["close"].pct_change()
                    df["market_return"] = spy_returns
                    df["beta"] = (
                        df["close"].pct_change().rolling(window=60).cov(spy_returns)
                        / spy_returns.rolling(window=60).var()
                    )

        except Exception as e:
            self.logger.warning(f"Failed to add market regime features: {e}")

        return df

    async def _add_sentiment_features(
        self, df: pd.DataFrame, symbol: str
    ) -> pd.DataFrame:
        """Add sentiment analysis features."""
        try:
            # Get news sentiment data
            sentiment_data = await self.db_manager.get_sentiment_data(
                symbol, df.index[0], df.index[-1]
            )
            if sentiment_data:
                sentiment_df = pd.DataFrame(sentiment_data)
                sentiment_df["timestamp"] = pd.to_datetime(sentiment_df["timestamp"])
                sentiment_df.set_index("timestamp", inplace=True)

                # Join sentiment data
                df = df.join(sentiment_df[["sentiment_score"]], how="left")
                df["sentiment_score"].fillna(0, inplace=True)

                # Sentiment moving averages
                df["sentiment_sma_5"] = df["sentiment_score"].rolling(window=5).mean()
                df["sentiment_momentum"] = df["sentiment_score"] - df["sentiment_sma_5"]

        except Exception as e:
            self.logger.warning(f"Failed to add sentiment features: {e}")

        return df

    async def _add_cross_asset_features(
        self, df: pd.DataFrame, symbol: str
    ) -> pd.DataFrame:
        """Add cross-asset correlation features."""
        try:
            # Get related assets data
            related_symbols = ["SPY", "QQQ", "GLD", "TLT", "DXY"]
            if symbol in related_symbols:
                related_symbols.remove(symbol)

            for related_symbol in related_symbols[
                :2
            ]:  # Limit to avoid too many features
                related_data = await self.db_manager.get_stock_data(
                    related_symbol, df.index[0], df.index[-1]
                )
                if related_data:
                    related_df = pd.DataFrame(related_data)
                    related_df["timestamp"] = pd.to_datetime(related_df["timestamp"])
                    related_df.set_index("timestamp", inplace=True)

                    related_returns = related_df["close"].pct_change()
                    symbol_returns = df["close"].pct_change()

                    # Rolling correlation
                    df[f"corr_{related_symbol}_30"] = symbol_returns.rolling(
                        window=30
                    ).corr(related_returns)

        except Exception as e:
            self.logger.warning(f"Failed to add cross-asset features: {e}")

        return df

    def _clean_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Clean and prepare features for ML models."""
        # Remove rows with too many NaN values
        df = df.dropna(thresh=len(df.columns) * 0.7)

        # Fill remaining NaN values
        df = df.fillna(method="ffill").fillna(method="bfill")

        # Remove infinite values
        df = df.replace([np.inf, -np.inf], np.nan)
        df = df.fillna(0)

        return df

    # =====================================================
    # MODEL TRAINING
    # =====================================================

    async def train_ensemble_models(
        self, symbol: str, prediction_horizon: int = 24
    ) -> Dict[str, Any]:
        """Train ensemble of ML models for price prediction."""
        try:
            # Get feature data
            df = await self.engineer_features(symbol, lookback_days=500)
            if df.empty:
                return {}

            # Prepare target variable (future returns)
            df["target"] = (
                df["close"].shift(-prediction_horizon).pct_change(prediction_horizon)
            )
            df = df.dropna()

            # Prepare features and target
            X = df[self.feature_columns]
            y = df["target"]

            # Split data
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42
            )

            # Scale features
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)

            self.scalers[symbol] = scaler

            # Train models
            models_performance = {}

            # Random Forest
            rf_model = RandomForestRegressor(**self.model_configs["random_forest"])
            rf_model.fit(X_train_scaled, y_train)
            rf_pred = rf_model.predict(X_test_scaled)
            rf_score = r2_score(y_test, rf_pred)

            models_performance["random_forest"] = {
                "model": rf_model,
                "r2_score": rf_score,
                "mse": mean_squared_error(y_test, rf_pred),
                "mae": mean_absolute_error(y_test, rf_pred),
            }

            # XGBoost
            xgb_model = xgb.XGBRegressor(**self.model_configs["xgboost"])
            xgb_model.fit(X_train_scaled, y_train)
            xgb_pred = xgb_model.predict(X_test_scaled)
            xgb_score = r2_score(y_test, xgb_pred)

            models_performance["xgboost"] = {
                "model": xgb_model,
                "r2_score": xgb_score,
                "mse": mean_squared_error(y_test, xgb_pred),
                "mae": mean_absolute_error(y_test, xgb_pred),
            }

            # LSTM (if TensorFlow available)
            if TENSORFLOW_AVAILABLE:
                lstm_performance = await self._train_lstm_model(
                    X_train_scaled, y_train, X_test_scaled, y_test, symbol
                )
                if lstm_performance:
                    models_performance["lstm"] = lstm_performance

            # Store models
            self.models[symbol] = models_performance
            self.model_performance[symbol] = {
                model_name: perf for model_name, perf in models_performance.items()
            }

            # Calculate feature importance
            rf_importance = dict(
                zip(self.feature_columns, rf_model.feature_importances_)
            )
            xgb_importance = dict(
                zip(self.feature_columns, xgb_model.feature_importances_)
            )

            # Average feature importance
            self.feature_importance[symbol] = {}
            for feature in self.feature_columns:
                avg_importance = (
                    rf_importance.get(feature, 0) + xgb_importance.get(feature, 0)
                ) / 2
                self.feature_importance[symbol][feature] = avg_importance

            # Save models
            await self._save_models(symbol)

            self.logger.info(f"Trained ensemble models for {symbol}")
            return models_performance

        except Exception as e:
            self.logger.error(f"Model training failed for {symbol}: {e}")
            return {}

    async def _train_lstm_model(
        self,
        X_train: np.ndarray,
        y_train: np.ndarray,
        X_test: np.ndarray,
        y_test: np.ndarray,
        symbol: str,
    ) -> Dict[str, Any]:
        """Train LSTM model for time series prediction."""
        try:
            config = self.model_configs["lstm"]
            seq_length = config["sequence_length"]

            # Reshape data for LSTM
            X_train_lstm = self._create_sequences(X_train, seq_length)
            X_test_lstm = self._create_sequences(X_test, seq_length)
            y_train_lstm = y_train[seq_length:]
            y_test_lstm = y_test[seq_length:]

            # Build LSTM model
            model = Sequential(
                [
                    LSTM(
                        config["lstm_units"],
                        return_sequences=True,
                        input_shape=(seq_length, X_train.shape[1]),
                    ),
                    Dropout(config["dropout"]),
                    LSTM(config["lstm_units"]),
                    Dropout(config["dropout"]),
                    Dense(25),
                    Dense(1),
                ]
            )

            model.compile(
                optimizer=Adam(learning_rate=0.001), loss="mse", metrics=["mae"]
            )

            # Train model
            history = model.fit(
                X_train_lstm,
                y_train_lstm,
                epochs=config["epochs"],
                batch_size=config["batch_size"],
                validation_split=0.2,
                verbose=0,
            )

            # Evaluate
            y_pred = model.predict(X_test_lstm)
            r2 = r2_score(y_test_lstm, y_pred)
            mse = mean_squared_error(y_test_lstm, y_pred)
            mae = mean_absolute_error(y_test_lstm, y_pred)

            return {
                "model": model,
                "r2_score": r2,
                "mse": mse,
                "mae": mae,
                "history": history.history,
            }

        except Exception as e:
            self.logger.error(f"LSTM training failed: {e}")
            return {}

    def _create_sequences(self, data: np.ndarray, seq_length: int) -> np.ndarray:
        """Create sequences for LSTM training."""
        sequences = []
        for i in range(len(data) - seq_length):
            sequences.append(data[i : i + seq_length])
        return np.array(sequences)

    # =====================================================
    # PREDICTION AND INFERENCE
    # =====================================================

    async def generate_predictions(
        self, symbol: str, prediction_horizon: int = 24
    ) -> List[MLPrediction]:
        """Generate predictions from all trained models."""
        try:
            predictions = []

            if symbol not in self.models:
                self.logger.warning(f"No trained models found for {symbol}")
                return predictions

            # Get latest features
            df = await self.engineer_features(symbol, lookback_days=100)
            if df.empty:
                return predictions

            # Get latest feature vector
            latest_features = df[self.feature_columns].iloc[-1:].values

            # Scale features
            if symbol in self.scalers:
                latest_features_scaled = self.scalers[symbol].transform(latest_features)
            else:
                return predictions

            # Generate predictions from each model
            for model_name, model_info in self.models[symbol].items():
                try:
                    model = model_info["model"]

                    if model_name == "lstm" and TENSORFLOW_AVAILABLE:
                        # LSTM requires sequence input
                        seq_length = self.model_configs["lstm"]["sequence_length"]
                        if len(df) >= seq_length:
                            sequence_data = (
                                df[self.feature_columns].iloc[-seq_length:].values
                            )
                            sequence_scaled = self.scalers[symbol].transform(
                                sequence_data
                            )
                            sequence_input = sequence_scaled.reshape(1, seq_length, -1)
                            pred_value = model.predict(sequence_input)[0][0]
                        else:
                            continue
                    else:
                        # Traditional ML models
                        pred_value = model.predict(latest_features_scaled)[0]

                    # Calculate confidence based on model performance
                    confidence = min(0.95, max(0.5, model_info["r2_score"]))

                    prediction = MLPrediction(
                        symbol=symbol,
                        model_name=model_name,
                        prediction_type="price_return",
                        predicted_value=float(pred_value),
                        confidence=confidence,
                        timestamp=datetime.now(),
                        features_used=self.feature_columns,
                        model_accuracy=model_info["r2_score"],
                    )
                    predictions.append(prediction)

                except Exception as e:
                    self.logger.error(f"Prediction failed for {model_name}: {e}")
                    continue

            # Sync predictions to InfluxDB
            await self._sync_predictions_to_influxdb(predictions)

            return predictions

        except Exception as e:
            self.logger.error(f"Prediction generation failed for {symbol}: {e}")
            return []

    async def generate_trading_signals(self, symbol: str) -> Optional[TradingSignal]:
        """Generate trading signals based on ensemble predictions."""
        try:
            predictions = await self.generate_predictions(symbol)
            if not predictions:
                return None

            # Ensemble prediction (weighted by confidence)
            total_weight = sum(pred.confidence for pred in predictions)
            if total_weight == 0:
                return None

            weighted_prediction = (
                sum(pred.predicted_value * pred.confidence for pred in predictions)
                / total_weight
            )
            avg_confidence = total_weight / len(predictions)

            # Determine signal
            if weighted_prediction > 0.02:  # 2% expected return threshold
                signal_type = "buy"
                strength = min(1.0, weighted_prediction * 10)  # Scale to 0-1
            elif weighted_prediction < -0.02:
                signal_type = "sell"
                strength = min(1.0, abs(weighted_prediction) * 10)
            else:
                signal_type = "hold"
                strength = 0.5

            # Calculate risk score
            risk_score = self._calculate_risk_score(predictions)

            signal = TradingSignal(
                symbol=symbol,
                signal_type=signal_type,
                strength=strength,
                confidence=avg_confidence,
                expected_return=weighted_prediction,
                risk_score=risk_score,
                timestamp=datetime.now(),
                model_ensemble=[pred.model_name for pred in predictions],
            )

            # Sync signal to InfluxDB
            await self._sync_signal_to_influxdb(signal)

            return signal

        except Exception as e:
            self.logger.error(f"Signal generation failed for {symbol}: {e}")
            return None

    def _calculate_risk_score(self, predictions: List[MLPrediction]) -> float:
        """Calculate risk score based on prediction variance."""
        if len(predictions) < 2:
            return 0.5

        values = [pred.predicted_value for pred in predictions]
        variance = np.var(values)

        # Normalize to 0-1 scale
        risk_score = min(1.0, variance * 100)
        return risk_score

    # =====================================================
    # SENTIMENT ANALYSIS
    # =====================================================

    async def analyze_sentiment(self, text: str, symbol: str = None) -> Dict[str, Any]:
        """Analyze sentiment of text using VADER."""
        try:
            if not SENTIMENT_AVAILABLE or not self.sentiment_analyzer:
                return {}

            scores = self.sentiment_analyzer.polarity_scores(text)

            sentiment_data = {
                "compound": scores["compound"],
                "positive": scores["pos"],
                "negative": scores["neg"],
                "neutral": scores["neu"],
                "timestamp": datetime.now(),
                "text_length": len(text),
            }

            if symbol:
                sentiment_data["symbol"] = symbol
                # Sync to InfluxDB
                await self._sync_sentiment_to_influxdb(sentiment_data)

            return sentiment_data

        except Exception as e:
            self.logger.error(f"Sentiment analysis failed: {e}")
            return {}

    # =====================================================
    # MODEL MANAGEMENT
    # =====================================================

    async def _save_models(self, symbol: str):
        """Save trained models to disk."""
        try:
            symbol_dir = self.model_directory / symbol
            symbol_dir.mkdir(exist_ok=True)

            # Save traditional ML models
            for model_name, model_info in self.models[symbol].items():
                if model_name != "lstm":  # LSTM saved separately
                    model_path = symbol_dir / f"{model_name}_model.pkl"
                    with open(model_path, "wb") as f:
                        pickle.dump(model_info["model"], f)

            # Save scaler
            if symbol in self.scalers:
                scaler_path = symbol_dir / "scaler.pkl"
                with open(scaler_path, "wb") as f:
                    pickle.dump(self.scalers[symbol], f)

            # Save feature importance
            if symbol in self.feature_importance:
                importance_path = symbol_dir / "feature_importance.json"
                with open(importance_path, "w") as f:
                    json.dump(self.feature_importance[symbol], f)

            # Save LSTM model (if TensorFlow available)
            if TENSORFLOW_AVAILABLE and "lstm" in self.models[symbol]:
                lstm_path = symbol_dir / "lstm_model"
                self.models[symbol]["lstm"]["model"].save(lstm_path)

            self.logger.info(f"Models saved for {symbol}")

        except Exception as e:
            self.logger.error(f"Failed to save models for {symbol}: {e}")

    # =====================================================
    # INFLUXDB INTEGRATION
    # =====================================================

    async def _sync_predictions_to_influxdb(self, predictions: List[MLPrediction]):
        """Sync predictions to InfluxDB."""
        try:
            for pred in predictions:
                await influx_sync.sync_ml_prediction(
                    symbol=pred.symbol,
                    model_name=pred.model_name,
                    prediction_type=pred.prediction_type,
                    predicted_value=pred.predicted_value,
                    confidence=pred.confidence,
                    model_accuracy=pred.model_accuracy,
                )
        except Exception as e:
            self.logger.error(f"Failed to sync predictions to InfluxDB: {e}")

    async def _sync_signal_to_influxdb(self, signal: TradingSignal):
        """Sync trading signal to InfluxDB."""
        try:
            await influx_sync.sync_trading_signal(
                symbol=signal.symbol,
                signal_type=signal.signal_type,
                strength=signal.strength,
                confidence=signal.confidence,
                expected_return=signal.expected_return,
                risk_score=signal.risk_score,
                model_ensemble=",".join(signal.model_ensemble),
            )
        except Exception as e:
            self.logger.error(f"Failed to sync signal to InfluxDB: {e}")

    async def _sync_sentiment_to_influxdb(self, sentiment_data: Dict[str, Any]):
        """Sync sentiment data to InfluxDB."""
        try:
            await influx_sync.sync_sentiment_analysis(
                symbol=sentiment_data.get("symbol", "market"),
                sentiment_score=sentiment_data["compound"],
                positive=sentiment_data["positive"],
                negative=sentiment_data["negative"],
                neutral=sentiment_data["neutral"],
            )
        except Exception as e:
            self.logger.error(f"Failed to sync sentiment to InfluxDB: {e}")


# Global instance
ai_ml_engine = None


async def initialize_ai_ml_engine(db_manager: DatabaseManager) -> AIMLEngine:
    """Initialize the global AI/ML engine."""
    global ai_ml_engine
    ai_ml_engine = AIMLEngine(db_manager)
    return ai_ml_engine
